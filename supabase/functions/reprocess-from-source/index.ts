import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2.75.1';

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

interface ReprocessRequest {
  document_id: string;
  category: string;
}

interface ChapterBoundary {
  chapter_number: number;
  chapter_title: string;
  start_page: number;
  end_page: number;
  full_text: string;
}

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  const encoder = new TextEncoder();
  const stream = new ReadableStream({
    async start(controller) {
      try {
        const { document_id, category } = await req.json() as ReprocessRequest;
        
        const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
        const supabaseKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
        const supabase = createClient(supabaseUrl, supabaseKey);

        const send = (data: any) => {
          controller.enqueue(encoder.encode(`data: ${JSON.stringify(data)}\n\n`));
        };

        send({ type: 'start', message: 'Starting re-processing from source PDF...' });

        // Get parent document info
        const { data: parentDoc, error: parentError } = await supabase
          .from('knowledge_base')
          .select('file_path, title')
          .eq('id', document_id)
          .single();

        if (parentError || !parentDoc?.file_path) {
          throw new Error('Could not find parent document');
        }

        send({ type: 'progress', message: 'Downloading PDF from storage...' });

        // Download PDF from storage
        const { data: fileData, error: downloadError } = await supabase.storage
          .from('knowledge-base')
          .download(parentDoc.file_path.replace('knowledge-base/', ''));

        if (downloadError || !fileData) {
          throw new Error(`Failed to download PDF: ${downloadError?.message}`);
        }

        send({ type: 'progress', message: 'Parsing PDF and detecting chapters...' });

        // Call parse-pdf function to extract full text
        const { data: parseData, error: parseError } = await supabase.functions.invoke('parse-pdf', {
          body: { fileData: await fileData.arrayBuffer() }
        });

        if (parseError || !parseData?.text) {
          throw new Error('Failed to parse PDF');
        }

        const fullText = parseData.text;
        const pages = parseData.pages || [];

        send({ type: 'progress', message: `Extracted ${pages.length} pages. Detecting chapter boundaries...` });

        // Detect chapter boundaries
        const chapters = await detectChapterBoundaries(pages, supabase, send);

        send({ type: 'progress', message: `Found ${chapters.length} chapters. Processing each chapter...` });

        // Delete old chunks
        await supabase
          .from('knowledge_base')
          .delete()
          .eq('parent_document_id', document_id);

        // Process each chapter
        let totalChunks = 0;
        for (const chapter of chapters) {
          send({ 
            type: 'progress', 
            message: `Processing Chapter ${chapter.chapter_number}: ${chapter.chapter_title}...` 
          });

          // Smart chunk the chapter text
          const chunks = smartChunk(chapter.full_text, chapter);

          // Extract metadata for each chunk
          for (let i = 0; i < chunks.length; i++) {
            const chunk = chunks[i];
            
            const metadata = await extractChapterMetadata(
              chunk.text,
              chapter,
              i === 0, // is first chunk
              supabase
            );

            // Insert chunk with verified chapter metadata
            const { error: insertError } = await supabase
              .from('knowledge_base')
              .insert({
                parent_document_id: document_id,
                title: parentDoc.title,
                content: chunk.text,
                category: category,
                chunk_index: totalChunks,
                chunk_metadata: {
                  ...metadata,
                  chapter_number: chapter.chapter_number,
                  chapter_title: chapter.chapter_title,
                  start_page: chunk.start_page,
                  end_page: chunk.end_page,
                  extraction_method: 'llm_reprocessed',
                  chapter_verified: true
                },
                embedding: null // Will be generated by separate process
              });

            if (insertError) {
              console.error('Insert error:', insertError);
            }

            totalChunks++;
          }

          send({ 
            type: 'progress', 
            message: `✓ Chapter ${chapter.chapter_number} complete (${chunks.length} chunks)` 
          });
        }

        send({ 
          type: 'complete', 
          message: `✅ Re-processed ${totalChunks} chunks across ${chapters.length} chapters`,
          chunks: totalChunks,
          chapters: chapters.length
        });

        controller.close();
      } catch (error) {
        console.error('Reprocess error:', error);
        const errorMessage = error instanceof Error ? error.message : 'Unknown error occurred';
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify({ 
            type: 'error', 
            message: errorMessage 
          })}\n\n`)
        );
        controller.close();
      }
    }
  });

  return new Response(stream, {
    headers: {
      ...corsHeaders,
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
    },
  });
});

// Detect chapter boundaries from page text
async function detectChapterBoundaries(
  pages: any[],
  supabase: any,
  send: (data: any) => void
): Promise<ChapterBoundary[]> {
  const chapters: ChapterBoundary[] = [];
  let currentChapter: Partial<ChapterBoundary> | null = null;

  for (let i = 0; i < pages.length; i++) {
    const pageText = pages[i].text || '';
    const pageNum = i + 1;

    // Look for chapter markers
    const chapterMatch = pageText.match(/^(?:CHAPTER|Chapter)\s+(\d+)[:\s]*(.+?)(?:\n|$)/mi);
    
    if (chapterMatch) {
      // Found a new chapter
      if (currentChapter && currentChapter.start_page) {
        // Close previous chapter
        currentChapter.end_page = pageNum - 1;
        currentChapter.full_text = extractTextRange(pages, currentChapter.start_page, currentChapter.end_page);
        chapters.push(currentChapter as ChapterBoundary);
      }

      // Start new chapter
      currentChapter = {
        chapter_number: parseInt(chapterMatch[1]),
        chapter_title: chapterMatch[2].trim(),
        start_page: pageNum,
        end_page: pageNum,
        full_text: ''
      };

      send({ 
        type: 'progress', 
        message: `Found Chapter ${currentChapter.chapter_number}: ${currentChapter.chapter_title} (page ${pageNum})` 
      });
    }
  }

  // Close last chapter
  if (currentChapter && currentChapter.start_page) {
    currentChapter.end_page = pages.length;
    currentChapter.full_text = extractTextRange(pages, currentChapter.start_page, currentChapter.end_page);
    chapters.push(currentChapter as ChapterBoundary);
  }

  return chapters;
}

// Extract text from page range
function extractTextRange(pages: any[], startPage: number, endPage: number): string {
  return pages
    .slice(startPage - 1, endPage)
    .map(p => p.text || '')
    .join('\n\n');
}

// Smart chunking based on chapter structure
function smartChunk(
  text: string, 
  chapter: ChapterBoundary
): Array<{ text: string; start_page: number; end_page: number }> {
  const chunks: Array<{ text: string; start_page: number; end_page: number }> = [];
  const maxChunkSize = 2000;
  const overlap = 200;

  // Try to split at paragraph boundaries
  const paragraphs = text.split(/\n\n+/);
  let currentChunk = '';
  let chunkStartPage = chapter.start_page;

  for (const para of paragraphs) {
    if (currentChunk.length + para.length > maxChunkSize && currentChunk.length > 0) {
      // Save current chunk
      chunks.push({
        text: currentChunk.trim(),
        start_page: chunkStartPage,
        end_page: chapter.end_page
      });
      
      // Start new chunk with overlap
      const overlapText = currentChunk.slice(-overlap);
      currentChunk = overlapText + '\n\n' + para;
      chunkStartPage = chapter.start_page;
    } else {
      currentChunk += (currentChunk ? '\n\n' : '') + para;
    }
  }

  // Add final chunk
  if (currentChunk.trim()) {
    chunks.push({
      text: currentChunk.trim(),
      start_page: chunkStartPage,
      end_page: chapter.end_page
    });
  }

  return chunks;
}

// Extract metadata using OpenAI
async function extractChapterMetadata(
  text: string,
  chapter: ChapterBoundary,
  isFirstChunk: boolean,
  supabase: any
): Promise<any> {
  const OPENAI_API_KEY = Deno.env.get("OPENAI_API_KEY");
  if (!OPENAI_API_KEY) {
    return { extraction_method: 'basic' };
  }

  const systemPrompt = `You are a textbook content analyzer. Extract structured metadata from the given text chunk.

CRITICAL CHAPTER RULES:
- This chunk is VERIFIED to be from Chapter ${chapter.chapter_number}: "${chapter.chapter_title}"
- DO NOT change or guess the chapter number
- Focus on extracting topics, keywords, and content details

Return a JSON object with:
- topics: array of 3-5 specific topics covered
- keywords: array of 5-10 key terms
- summary: 2-3 sentence summary
- content_type: "concept" | "example" | "exercise" | "quiz" | "definition"
- complexity: "beginner" | "intermediate" | "advanced"
- answers_questions: boolean (does this contain Q&A?)
- is_quiz_question: boolean`;

  const userPrompt = `Chapter ${chapter.chapter_number}: ${chapter.chapter_title}

Text to analyze:
${text.substring(0, 1500)}`;

  try {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: userPrompt }
        ],
        tools: [
          {
            type: 'function',
            function: {
              name: 'extract_metadata',
              parameters: {
                type: 'object',
                properties: {
                  topics: { type: 'array', items: { type: 'string' } },
                  keywords: { type: 'array', items: { type: 'string' } },
                  summary: { type: 'string' },
                  content_type: { type: 'string' },
                  complexity: { type: 'string' },
                  answers_questions: { type: 'boolean' },
                  is_quiz_question: { type: 'boolean' }
                },
                required: ['topics', 'keywords', 'summary', 'content_type', 'complexity']
              }
            }
          }
        ],
        tool_choice: { type: 'function', function: { name: 'extract_metadata' } }
      }),
    });

    if (response.ok) {
      const data = await response.json();
      const toolCall = data.choices[0]?.message?.tool_calls?.[0];
      if (toolCall) {
        return JSON.parse(toolCall.function.arguments);
      }
    }
  } catch (error) {
    console.error('Metadata extraction error:', error);
  }

  return { extraction_method: 'basic' };
}
